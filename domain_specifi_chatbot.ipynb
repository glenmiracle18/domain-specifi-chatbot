{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "lUNXVlkJA1kP"
      },
      "outputs": [],
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U datasets\n",
        "!pip install -U accelerate\n",
        "!pip install -U peft\n",
        "!pip install -U trl\n",
        "!pip install -U bitsandbytes\n",
        "!pip install huggingface_hub[hf_xet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AK0htE6RBRa5"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Load tokenizer & model\n",
        "# model_id = \"meta-llama/Llama-2-7b-hf\"\n",
        "peft_model = \"FinGPT/fingpt-mt_llama2-7b_lora\"\n",
        "\n",
        "model_dir = \"facebook/opt-350m\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_dir,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "zUNel3RDRhPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "RDgjbA6YRfyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_prompt_style=\"\"\"\n",
        "<|im_start|>system<|im_sep|>\n",
        "Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "<|im_end|>\n",
        "<|im_start|>user<|im_sep|>\n",
        "{}<|im_end|>\n",
        "<|im_start|>assistant<|im_sep|>\n",
        "<think>\n",
        "{}\n",
        "</think>\n",
        "{}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sUYg0DeESinI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    inputs = examples[\"Open-ended Verifiable Question\"]\n",
        "    complex_cots = examples[\"Complex_CoT\"]\n",
        "    outputs = examples[\"Response\"]\n",
        "    texts = []\n",
        "    for input, cot, output in zip(inputs,complex_cots,outputs):\n",
        "        text = train_prompt_style.format(input,cot,output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\n",
        "        \"text\": texts,\n",
        "    }"
      ],
      "metadata": {
        "id": "So8Ezxs3ShGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"TheFinAI/Fino1_Reasoning_Path_FinQA\", split=\"train[0:1000]\", trust_remote_code=True\n",
        ")\n",
        "dataset = dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        ")\n",
        "dataset[\"text\"][20]"
      ],
      "metadata": {
        "id": "8CmR_Y0LRfQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")"
      ],
      "metadata": {
        "id": "4rrVKLhDReye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_prompt_style = \"\"\"\n",
        "<|im_start|>system<|im_sep|>\n",
        "Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "<|im_end|>\n",
        "<|im_start|>user<|im_sep|>\n",
        "{}<|im_end|>\n",
        "<|im_start|>assistant<|im_sep|>\n",
        "<think>\n",
        "{}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "EneT0gpOUzVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should return True\n",
        "print(torch.cuda.get_device_name(0))  # Should show the GPU name\n"
      ],
      "metadata": {
        "id": "CARuOYeKWPnU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = dataset[20]['Open-ended Verifiable Question']\n",
        "inputs = tokenizer(\n",
        "    [inference_prompt_style.format(question, \"\") + tokenizer.eos_token],\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    attention_mask=inputs.attention_mask,\n",
        "    max_new_tokens=250,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    use_cache=True,\n",
        ")\n",
        "response = tokenizer.batch_decode(outputs)\n",
        "print(response[0].split(\"<|im_start|>assistant<|im_sep|>\")[1])"
      ],
      "metadata": {
        "id": "A6DxmRjOUy1y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# LoRA config\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,                           # Scaling factor for LoRA\n",
        "    lora_dropout=0.05,                       # Add slight dropout for regularization\n",
        "    r=64,                                    # Rank of the LoRA update matrices\n",
        "    bias=\"none\",                             # No bias reparameterization\n",
        "    task_type=\"CAUSAL_LM\",                   # Task type: Causal Language Modeling\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],  # Target modules for LoRA\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "metadata": {
        "id": "SN9xS9OSWWgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "\n",
        "# Training Arguments\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"output\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=0.2,\n",
        "    warmup_steps=10,\n",
        "    logging_strategy=\"steps\",\n",
        "    learning_rate=2e-4,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    group_by_length=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_config,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "_1s2LPrjWV6l",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc, torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "# facebook/opt-350m\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Set the padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "-XVY-DNxUyOy",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: can you save it to this dir ./fine_tuned_mode\n",
        "\n",
        "# Save the model\n",
        "output_dir = \"./fine_tuned_model\"\n",
        "trainer.save_model(output_dir)"
      ],
      "metadata": {
        "id": "UXj5Tk3xXnc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing fine tuned model"
      ],
      "metadata": {
        "id": "T3nOn1ORShK7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "793bfa26"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load the base model and tokenizer\n",
        "model_dir = \"facebook/opt-350m\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_dir,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load the fine-tuned LoRA adapter\n",
        "peft_model_dir = \"./fine_tuned_model\" # Or wherever you saved your model\n",
        "model = PeftModel.from_pretrained(base_model, peft_model_dir)\n",
        "\n",
        "# Merge the LoRA weights with the base model for easier inference\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Set the padding token if it was set during training\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Move model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9a64d951"
      },
      "source": [
        "# Define your test prompts\n",
        "test_prompts = [\n",
        "    \"Please answer the given financial question based on the context. Context: In the fiscal year ending December 31, 2023, Company X reported a net income of $500 million. The total revenue for the same period was $2.5 billion. Question: What was the net profit margin for Company X in 2023?\",\n",
        "    # Add more prompts as needed\n",
        "]\n",
        "\n",
        "# Prepare prompts with the inference style and tokenize\n",
        "inference_prompt_style = \"\"\"\n",
        "<|im_start|>system<|im_sep|>\n",
        "Below is an instruction that describes a task, paired with an input that provides further context.\n",
        "Write a response that appropriately completes the request.\n",
        "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
        "<|im_end|>\n",
        "<|im_start|>user<|im_sep|>\n",
        "{}<|im_end|>\n",
        "<|im_start|>assistant<|im_sep|>\n",
        "<think>\n",
        "{}\n",
        "\"\"\"\n",
        "\n",
        "inputs = [inference_prompt_style.format(prompt, \"\") + tokenizer.eos_token for prompt in test_prompts]\n",
        "inputs = tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad(): # Disable gradient calculation for inference\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_new_tokens=250,  # Adjust as needed\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id, # Use the correct padding token\n",
        "        use_cache=True,\n",
        "    )\n",
        "\n",
        "responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "for i, response in enumerate(responses):\n",
        "    print(f\"Prompt {i+1}:\")\n",
        "    # Split the response to show only the assistant's part\n",
        "    assistant_response = response.split(\"<|im_start|>assistant<|im_sep|>\")\n",
        "    if len(assistant_response) > 1:\n",
        "        print(assistant_response[1])\n",
        "    else:\n",
        "        print(response) # Print the whole response if the split didn't work as expected\n",
        "    print(\"-\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e391b87f"
      },
      "source": [
        "login(token=hf_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40e36b53"
      },
      "source": [
        "# Replace \"your-username/your-model-name\" with your desired repository ID\n",
        "repo_id = \"your-username/my-fine-tuned-financial-model\"\n",
        "\n",
        "model.push_to_hub(repo_id)\n",
        "tokenizer.push_to_hub(repo_id)\n",
        "\n",
        "print(f\"Model and tokenizer pushed to https://huggingface.co/{repo_id}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}